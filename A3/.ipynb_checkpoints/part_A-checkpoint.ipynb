{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/11 19:56:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/11 19:56:34 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n"
     ]
    }
   ],
   "source": [
    "spark_session = (\n",
    "    SparkSession.builder\n",
    "    .master(\"spark://192.168.2.156:7077\")\n",
    "    .appName(\"Alexnader_partA\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True)\n",
    "    .config(\"spark.shuffle.service.enabled\", True)\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\")\n",
    "    .config(\"spark.cores.max\", 2)\n",
    "    .getOrCreate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark_session.sparkContext  # SparkContext\n",
    "rdd_en = sc.textFile(\"hdfs://192.168.2.156:9000/data/europarl/europarl-v7.sv-en.en\")\n",
    "rdd_other = sc.textFile(\"hdfs://192.168.2.156:9000/data/europarl/europarl-v7.sv-en.sv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/11 19:56:55 ERROR TaskSchedulerImpl: Lost executor 0 on 192.168.2.134: Unable to create executor due to Unable to register with external shuffle server due to : Failed to connect to /192.168.2.134:7337\n",
      "25/03/11 19:57:10 ERROR TaskSchedulerImpl: Lost executor 1 on 192.168.2.134: Unable to create executor due to Unable to register with external shuffle server due to : Failed to connect to /192.168.2.134:7337\n",
      "25/03/11 19:57:10 ERROR TaskSchedulerImpl: Lost executor 2 on 192.168.2.131: Unable to create executor due to Unable to register with external shuffle server due to : Failed to connect to /192.168.2.131:7337\n",
      "25/03/11 19:57:24 ERROR TaskSchedulerImpl: Lost executor 3 on 192.168.2.134: Unable to create executor due to Unable to register with external shuffle server due to : Failed to connect to /192.168.2.134:7337\n",
      "25/03/11 19:57:24 ERROR TaskSchedulerImpl: Lost executor 4 on 192.168.2.131: Unable to create executor due to Unable to register with external shuffle server due to : Failed to connect to /192.168.2.131:7337\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_en = rdd_en.count()\n",
    "count_other = rdd_other.count()\n",
    "print(\"English lines:\", count_en)\n",
    "print(\"Other language lines:\", count_other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Partitions (English):\", rdd_en.getNumPartitions())\n",
    "print(\"Partitions (Other):\", rdd_other.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_line(line):\n",
    "    line = line.lower()\n",
    "    return line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_en_clean = rdd_en.map(preprocess_line)\n",
    "rdd_other_clean = rdd_other.map(preprocess_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English sample:\", rdd_en_clean.take(5))\n",
    "print(\"Other language sample:\", rdd_other_clean.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts_en = (\n",
    "    rdd_en_clean\n",
    "    .flatMap(lambda words: words)     # flatten all tokens\n",
    "    .map(lambda w: (w, 1))            # create (word, 1) pairs\n",
    "    .reduceByKey(lambda a, b: a + b)  # sum the counts\n",
    ")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_en = word_counts_en.takeOrdered(10, key=lambda x: -x[1])\n",
    "print(\"Top 10 words (English):\", top_10_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_en_indexed = rdd_en_clean.zipWithIndex().map(lambda x: (x[1], x[0]))\n",
    "rdd_other_indexed = rdd_other_clean.zipWithIndex().map(lambda x: (x[1], x[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd = rdd_en_indexed.join(rdd_other_indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd_nonempty = joined_rdd.filter(\n",
    "    lambda x: len(x[1][0]) > 0 and len(x[1][1]) > 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd_short = joined_rdd_nonempty.filter(\n",
    "    lambda x: len(x[1][0]) < 10 and len(x[1][1]) < 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_rdd_same_length = joined_rdd_short.filter(\n",
    "    lambda x: len(x[1][0]) == len(x[1][1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (index, ([\"hello\", \"world\"], [\"hej\", \"världen\"]))\n",
    "# => [(\"hello\", \"hej\"), (\"world\", \"världen\")]\n",
    "word_pairs_rdd = joined_rdd_same_length.flatMap(\n",
    "    lambda x: zip(x[1][0], x[1][1])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_counts = (\n",
    "    word_pairs_rdd\n",
    "    .map(lambda pair: (pair, 1))\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_pairs = pair_counts.takeOrdered(20, key=lambda x: -x[1])\n",
    "print(\"Most common word-translation pairs:\", top_word_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
